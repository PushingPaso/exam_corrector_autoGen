import os
import re
import sys
import json
from dataclasses import dataclass
from enum import Enum
from pathlib import Path
from autogen_core.models import UserMessage

from pydantic import BaseModel, Field

from exam import DIR_ROOT
from exam import get_questions_store
from exam.llm_provider import get_llm
from exam.solution import Answer

OUTPUT_FILE = os.getenv("OUTPUT_FILE", None)
if OUTPUT_FILE:
    OUTPUT_FILE = open(OUTPUT_FILE, "w", encoding="utf-8")
else:
    OUTPUT_FILE = sys.stdout

ALL_QUESTIONS = get_questions_store()
PATTERN_QUESTION_FOLDER = re.compile(r"^Q\d+\s+-\s+(\w+-\d+)$")
FILE_TEMPLATE = DIR_ROOT / "exam" / "assess" / "prompt-template.txt"
TEMPLATE = FILE_TEMPLATE.read_text(encoding="utf-8")


class FeatureType(str, Enum):
    """Enumeration of feature types that can be assessed in a question's answer."""
    CORE = "core"
    DETAILS_IMPORTANT = "important detail"


@dataclass(frozen=True)
class Feature:
    type: FeatureType
    description: str

    @property
    def verb_ideal(self) -> str:
        return "should be present"

    @property
    def verb_actual(self) -> str:
        return "is actually present"

    @property
    def is_core(self) -> bool:
        """Determines if this feature is core (essential)."""
        return self.type == FeatureType.CORE


def enumerate_features(answer: Answer):
    """Enumerates the features to be assessed."""
    if not answer:
        return
    i = 0

    for core_item in answer.core:
        yield i, Feature(type=FeatureType.CORE, description=core_item)
        i += 1

    for detail in answer.details_important:
        yield i, Feature(type=FeatureType.DETAILS_IMPORTANT, description=detail)
        i += 1


class FeatureAssessment(BaseModel):
    satisfied: bool = Field(description="Whether the feature is present in the answer")
    motivation: str = Field(description="Explanation of why the feature is present or not")


class Assessor:
    """
    Class for the structured assessment of student answers.
    Includes assessment logic and result saving.
    """

    def __init__(self, evaluations_dir=None):
        """
        Initializes the assessor.

        Args:
            evaluations_dir: Directory to save evaluations (default: DIR_ROOT/evaluations)
        """
        from exam.llm_provider import get_llm

        if evaluations_dir is None:
            self.evaluations_dir = DIR_ROOT / "evaluations"
        else:
            self.evaluations_dir = Path(evaluations_dir)

        self.evaluations_dir.mkdir(parents=True, exist_ok=True)

    async def assess_single_answer(
            self,
            question,
            checklist,
            student_response: str,
            max_score: float
    ) -> dict:
        """
        Evaluates a single student answer.

        Args:
            question: Question object with id and text
            checklist: Answer object with core and details_important
            student_response: Text of the student's response
            max_score: Maximum score for this question

        Returns:
            dict containing status, score, max_score, statistics, breakdown, and feature assessments.
        """
        if not student_response or student_response.strip() == '-':
            return {
                "status": "no_response",
                "score": 0.0,
                "max_score": max_score
            }

        try:
            feature_assessments_list = []
            feature_assessments_dict = {}

            for index, feature in enumerate_features(checklist):
                prompt = TEMPLATE.format(
                    class_name="FeatureAssessment",
                    question=question.text,
                    feature_type=feature.type.value,
                    feature_verb_ideal=feature.verb_ideal,
                    feature_verb_actual=feature.verb_actual,
                    feature=feature.description,
                    answer=student_response
                )

                llm = get_llm()

                json_prompt = f"""{prompt}

                You must respond with ONLY a valid JSON object matching this schema:
                {{
                    "satisfied": boolean,
                    "motivation": string
                }}

                Do not include any additional fields or text outside the JSON object."""

                response = await llm.create(
                    messages=[UserMessage(content=json_prompt, source="user")]
                )

                if hasattr(response, 'content'):
                    json_str = response.content
                elif isinstance(response, str):
                    json_str = response
                else:
                    json_str = response.content

                json_str = json_str.strip()
                if json_str.startswith('```json'):
                    json_str = json_str.replace('```json', '').replace('```', '').strip()
                if json_str.startswith('```'):
                    json_str = json_str.replace('```', '').strip()

                result_dict = json.loads(json_str)
                result = FeatureAssessment(**result_dict)

                feature_assessments_list.append({
                    "feature": feature.description,
                    "feature_type": feature.type.name,
                    "satisfied": result.satisfied,
                    "motivation": result.motivation
                })

                feature_assessments_dict[feature] = result

            score, breakdown, stats = self.calculate_score(
                feature_assessments_dict,
                max_score
            )

            return {
                "status": "assessed",
                "score": score,
                "max_score": max_score,
                "statistics": stats,
                "breakdown": breakdown,
                "feature_assessments": feature_assessments_list
            }

        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "score": 0.0,
                "max_score": max_score
            }

    async def assess_student_exam(
            self,
            student_email: str,
            exam_questions: list,
            student_responses: dict,
            questions_store,
            context,
            save_results: bool = True,
            original_grades: dict = None
    ) -> dict:
        """
        Evaluates all answers for a student.

        Args:
            student_email: Student email
            exam_questions: List of dicts with question info
            student_responses: Dict {question_number: response_text}
            questions_store: QuestionsStore instance
            context: AssessmentContext to access checklists
            save_results: If True, saves results to file
            original_grades: True grades

        Returns:
            dict containing assessment results and saved file paths.
        """
        from exam.solution import load_cache as load_answer_cache

        assessments = []
        total_score = 0.0
        total_max_score = 0.0

        for question_info in exam_questions:
            question_num = int(question_info["number"].replace("Question ", ""))

            if question_num not in student_responses:
                assessments.append({
                    "question_number": question_num,
                    "question_id": question_info["id"],
                    "question_text": question_info.get("text", ""),
                    "status": "no_response",
                    "score": 0.0,
                    "max_score": question_info["score"]
                })
                total_max_score += question_info["score"]
                continue

            try:
                question = questions_store.question(question_info["id"])
                checklist = context.get_checklist(question_info["id"])

                if not checklist:
                    checklist = load_answer_cache(question)
                    if checklist:
                        context.store_checklist(question_info["id"], checklist)

                if not checklist:
                    raise ValueError(f"No checklist found for question {question_info['id']}")

                response_text = student_responses[question_num]

                assessment = await self.assess_single_answer(
                    question=question,
                    checklist=checklist,
                    student_response=response_text,
                    max_score=question_info["score"]
                )

                assessment.update({
                    "question_number": question_num,
                    "question_id": question_info["id"],
                    "question_text": question.text,
                    "student_response": response_text
                })

                assessments.append(assessment)

                total_score += assessment["score"]
                total_max_score += question_info["score"]

            except Exception as e:
                assessments.append({
                    "question_number": question_num,
                    "question_id": question_info["id"],
                    "question_text": question_info.get("text", ""),
                    "status": "error",
                    "error": str(e),
                    "score": 0.0,
                    "max_score": question_info["score"]
                })
                total_max_score += question_info["score"]

        result = {
            "student_email": student_email,
            "calculated_score": total_score,
            "max_score": total_max_score,
            "percentage": round((total_score / total_max_score * 100) if total_max_score > 0 else 0, 1),
            "scoring_system": "70% Core + 30% Important_Details",
            "assessments": assessments,
            "original_grades": original_grades if original_grades else {}
        }

        if save_results:
            saved_files = self._save_assessment_results(student_email, result, exam_questions)
            result["saved_files"] = saved_files

        return result

    def _save_assessment_results(self, student_email: str, result: dict, exam_questions: list) -> dict:
        """
        Saves assessment results to files.

        Args:
            student_email: Student email
            result: Dictionary with assessment results
            exam_questions: List of exam questions

        Returns:
            dict with paths of saved files
        """
        student_dir = self.evaluations_dir / student_email
        student_dir.mkdir(parents=True, exist_ok=True)

        assessment_file = student_dir / "assessment.json"
        with open(assessment_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)

        summary_file = student_dir / "summary.txt"
        summary_content = self._generate_summary_text(student_email, result, exam_questions)

        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(summary_content)

        return {
            "assessment": str(assessment_file),
            "summary": str(summary_file)
        }

    def _generate_summary_text(self, student_email: str, result: dict, exam_questions: list) -> str:
        """
        Generates the readable summary text.
        """
        lines = []
        lines.append("STUDENT ASSESSMENT SUMMARY")
        lines.append("=" * 70)
        lines.append("")
        lines.append(f"Student: {student_email}")
        lines.append(f"Calculated Score: {result['calculated_score']:.2f}/{result['max_score']}")
        lines.append(f"Calculated Percentage: {result['percentage']}%")

        original_grades = result.get('original_grades', {})

        if original_grades:
            original_total = original_grades.get("total_grade", 0)
            lines.append(f"Original Moodle Grade: {original_total:.2f}/27.00")

            score_diff = result['calculated_score'] - original_total
            diff_text = f"Difference: {score_diff:+.2f} "
            if abs(score_diff) < 0.5:
                diff_text += "( Very close)"
            elif abs(score_diff) < 2.0:
                diff_text += "( Reasonable)"

        lines.append(f"Scoring System: {result['scoring_system']}")
        lines.append("")
        lines.append("=" * 70)
        lines.append("")

        for assessment in result["assessments"]:
            question_num = assessment['question_number']
            lines.append(f"Question {question_num}: {assessment['question_id']}")
            lines.append("-" * 70)

            if assessment['status'] == 'assessed':
                lines.append(f"Calculated Score: {assessment['score']:.2f}/{assessment['max_score']}")

                if original_grades and 'question_grades' in original_grades:
                    orig_q_grade = original_grades['question_grades'].get(question_num)
                    if orig_q_grade is not None:
                        diff = assessment['score'] - orig_q_grade
                        lines.append(f"Original Grade: {orig_q_grade:.2f}/{assessment['max_score']}")
                        lines.append(f"Difference: {diff:+.2f}")

                lines.append(f"Breakdown: {assessment['breakdown']}")
                lines.append("")

                core_features = [fa for fa in assessment['feature_assessments']
                                 if fa['feature_type'] == 'CORE']
                important_features = [fa for fa in assessment['feature_assessments']
                                      if fa['feature_type'] == 'DETAILS_IMPORTANT']

                if core_features:
                    lines.append("CORE Elements:")
                    for fa in core_features:
                        status = "[OK]" if fa['satisfied'] else "[MISSING]"
                        lines.append(f"  {status} {fa['feature']}")
                        lines.append(f"       {fa['motivation']}")
                        lines.append("")

                if important_features:
                    lines.append("Important Details:")
                    for fa in important_features:
                        status = "[OK]" if fa['satisfied'] else "[MISSING]"
                        lines.append(f"  {status} {fa['feature']}")
                        lines.append(f"       {fa['motivation']}")
                        lines.append("")

            else:
                lines.append(f"Status: {assessment['status']}")
                if 'error' in assessment:
                    lines.append(f"Error: {assessment['error']}")

            lines.append("")
            lines.append("=" * 70)
            lines.append("")

        return "\n".join(lines)

    def calculate_score(self, assessments: dict, max_score: float) -> tuple[float, str, dict]:
        """
        Calculates the score from an assessment dictionary.
        System:
        - 70% Core + 30% Important (if both present)
        - 100% Core (if Important missing)
        - 100% Important (if Core missing - rare)

        Args:
            assessments: dict[Feature, FeatureAssessment]
            max_score: Maximum score for the question

        Returns:
            tuple(score, breakdown, stats): Score, explanation, and detailed statistics
        """
        if not assessments:
            return 0.0, "No features assessed", {}

        core_total = sum(1 for f in assessments if f.type == FeatureType.CORE)
        core_satisfied = sum(1 for f, a in assessments.items()
                             if f.type == FeatureType.CORE and a.satisfied)

        important_total = sum(1 for f in assessments if f.type == FeatureType.DETAILS_IMPORTANT)
        important_satisfied = sum(1 for f, a in assessments.items()
                                  if f.type == FeatureType.DETAILS_IMPORTANT and a.satisfied)

        if core_total > 0 and important_total > 0:
            core_weight = 0.70
            important_weight = 0.30
            scoring_system = "70% Core + 30% Important"
        elif core_total > 0:
            core_weight = 1.0
            important_weight = 0.0
            scoring_system = "100% Core (no Important details)"
        elif important_total > 0:
            core_weight = 0.0
            important_weight = 1.0
            scoring_system = "100% Important (no Core - unusual)"
        else:
            return 0.0, "No features assessed", {}

        core_percentage = (core_satisfied / core_total * core_weight) if core_total > 0 else 0.0
        important_percentage = (
                    important_satisfied / important_total * important_weight) if important_total > 0 else 0.0

        final_percentage = core_percentage + important_percentage

        score = round(final_percentage * max_score, 2)

        breakdown_parts = []

        if core_total > 0:
            core_raw_pct = (core_satisfied / core_total * 100)
            core_weighted_pct = (core_percentage * 100)
            breakdown_parts.append(
                f"Core: {core_satisfied}/{core_total} "
                f"({core_raw_pct:.0f}% -> {core_weighted_pct:.0f}%)"
            )

        if important_total > 0:
            important_raw_pct = (important_satisfied / important_total * 100)
            important_weighted_pct = (important_percentage * 100)
            breakdown_parts.append(
                f"Important: {important_satisfied}/{important_total} "
                f"({important_raw_pct:.0f}% -> {important_weighted_pct:.0f}%)"
            )

        breakdown = " + ".join(breakdown_parts)
        breakdown += f" = {final_percentage * 100:.0f}% of {max_score} = {score}"
        breakdown += f" [{scoring_system}]"

        stats = {
            "core": {
                "total": core_total,
                "satisfied": core_satisfied,
                "percentage": round((core_satisfied / core_total * 100) if core_total > 0 else 0, 1),
                "weight": core_weight
            },
            "details_important": {
                "total": important_total,
                "satisfied": important_satisfied,
                "percentage": round((important_satisfied / important_total * 100) if important_total > 0 else 0, 1),
                "weight": important_weight
            },
            "scoring_system": scoring_system
        }

        return score, breakdown, stats